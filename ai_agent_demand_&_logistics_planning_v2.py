# -*- coding: utf-8 -*-
"""AI Agent - Demand & Logistics Planning V2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYZPi34SlnXZ9gVpMlVnzMkPHnpViQev

# Section 1: Import Libraries and Set Up Environment
"""

# SECTION 1: Import Required Libraries
import pandas as pd
import numpy as np
import os
from datetime import datetime
from dateutil.relativedelta import relativedelta
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import warnings
from google.colab import drive
from xgboost import XGBRegressor
import multiprocessing
from prophet import Prophet
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from scipy.stats import linregress
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
from scipy.stats import linregress
from collections import namedtuple

warnings.filterwarnings("ignore")

"""# Section 2: Load and Combine All Excel Files"""

# SECTION 2: Load All Excel Files in the Folder
# Assumes all relevant Excel files are in the same folder in Google Drive/Colab environment

drive.mount("/content/drive", force_remount=True)
folder_path = '/content/drive/My Drive/Sample Files/In Market Sales Data'  # Adjust as needed
all_data = []

for file in os.listdir(folder_path):
    if file.endswith('.xlsx'):
        df = pd.read_excel(os.path.join(folder_path, file))
        all_data.append(df)

# Combine all files into a single DataFrame
df_raw = pd.concat(all_data, ignore_index=True)
print("‚úÖ Raw data loaded with shape:", df_raw.shape)

"""# Section 3: Clean & Prepare Transaction & Master Data"""

# SECTION 3: Clean and Prepare Data (with Monthly Aggregation) and Master Data with Forecastability Segmentation

df_raw.columns = df_raw.columns.str.strip()
df_raw['PERIOD'] = pd.to_datetime(
    df_raw['PERIOD YEAR'].astype(str) + '-' + df_raw['PERIOD ID'].astype(str).str.zfill(2) + '-01'
)
df_raw['CY_Total'] = df_raw['CY QTY'].fillna(0) + df_raw['CY BNS'].fillna(0)

df = df_raw[['ALT COUNTRY KEY', 'JP CUST TYPE', 'Product ID', 'PERIOD', 'CY_Total', 'NET']].copy()
df.rename(columns={
    'JP CUST TYPE': 'CHANNEL',
    'ALT COUNTRY KEY': 'COUNTRY',
    'Product ID': 'PRODUCT_ID',
    'PERIOD': 'DATE',
    'CY_Total': 'CY_QTY',
    'NET': 'ACT_VALUE'
}, inplace=True)

monthly_df = (
    df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE'], as_index=False)
    .agg({'CY_QTY': 'sum', 'ACT_VALUE': 'sum'})
)

# === Pad with Missing Months ===
def pad_monthly_series(df):
    padded_list = []
    for key, group in df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID']):
        group = group.set_index('DATE').sort_index()
        full_range = pd.date_range(start=group.index.min(), end=group.index.max(), freq='MS')
        group = group.reindex(full_range).fillna(0)
        group['DATE'] = group.index
        group['COUNTRY'], group['CHANNEL'], group['PRODUCT_ID'] = key
        padded_list.append(group.reset_index(drop=True))
    return pd.concat(padded_list, ignore_index=True)

monthly_df = pad_monthly_series(monthly_df)

# === Compute FMR and XYZ Classifications ===
def classify_fmr(sales):
    active_months = (sales > 0).sum()
    if active_months >= 24:
        return 'F'
    elif active_months >= 12:
        return 'M'
    else:
        return 'R'

def classify_xyz(sales):
    if sales.mean() == 0:
        return 'Z'
    cov = sales.std() / sales.mean()
    if cov < 0.5:
        return 'X'
    elif cov < 1.0:
        return 'Y'
    else:
        return 'Z'

forecastability_tags = []
for key, group in monthly_df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID']):
    fmr = classify_fmr(group['CY_QTY'])
    xyz = classify_xyz(group['CY_QTY'])
    if fmr == 'F' and xyz == 'X':
        level = 'High'
    elif fmr == 'R' and xyz == 'Z':
        level = 'Low'
    elif fmr == 'F' or xyz == 'X':
        level = 'Medium'
    else:
        level = 'Low'
    forecastability_tags.append({
        'COUNTRY': key[0], 'CHANNEL': key[1], 'PRODUCT_ID': key[2],
        'FMR': fmr, 'XYZ': xyz, 'Forecastability_Level': level
    })

forecastability_df = pd.DataFrame(forecastability_tags)

# === Generate Master Data Table ===
master_cols = [
    'CLUSTER NAME', 'DIVISION', 'ALT COUNTRY KEY', 'CATEGORY ID', 'PRODUCT ID',
    'PRODUCT', 'PROD GROUP', 'JP CUST TYPE', 'Product ID'
]
master_data = df_raw[master_cols].drop_duplicates()

# Join with forecastability info
master_data = master_data.rename(columns={
    'ALT COUNTRY KEY': 'COUNTRY',
    'JP CUST TYPE': 'CHANNEL',
    'Product ID': 'PRODUCT_ID'
})

master_enriched = master_data.merge(forecastability_df, on=['COUNTRY', 'CHANNEL', 'PRODUCT_ID'], how='left')
master_enriched.head()

print("‚úÖ Monthly data aggregated with shape:", monthly_df.shape)
print("‚úÖ Master Data with forecastability segementations")

"""# Section 4: Build Forecast Base ‚Äì Group & Resample"""

# SECTION 4: Prepare Data for Forecasting (Monthly Level)

# Use the monthly aggregated dataset
monthly_df['DATE'] = pd.to_datetime(monthly_df['DATE'])
monthly_df['CY_QTY'] = pd.to_numeric(monthly_df['CY_QTY'], errors='coerce').fillna(0)
monthly_df['ACT_VALUE'] = pd.to_numeric(monthly_df['ACT_VALUE'], errors='coerce').fillna(0)

# Define unique groups to forecast for
group_cols = ['COUNTRY', 'CHANNEL', 'PRODUCT_ID']
grouped_data = monthly_df.groupby(group_cols)

# Containers for forecasts and metrics
final_forecasts = []
model_metrics = []

print(f"üîç Forecasting for {len(grouped_data)} product-market groups at monthly level...")

"""# Section 5: Forecast Generation"""

# SECTION 5 - FORECAST GENERATION (Prophet Only, Monthly)



# ========= Helper Functions ========= #

def correct_outliers(series):
    """Winsorize extreme outliers."""
    lower = series.quantile(0.05)
    upper = series.quantile(0.95)
    return series.clip(lower=lower, upper=upper)

def forecast_single_group(group_key, group_df, forecast_horizon=36):  # Monthly horizon
    try:
        result_list = []

        group_df = group_df.sort_values("DATE")
        group_df['CY_QTY'] = correct_outliers(group_df['CY_QTY']).rolling(window=3, min_periods=1).mean()

        if group_df['CY_QTY'].sum() == 0 or group_df.shape[0] < 6:
            return None

        train = group_df.copy()

        prophet_df = train[['DATE', 'CY_QTY']].rename(columns={'DATE': 'ds', 'CY_QTY': 'y'})

        prophet_params = [
            {"changepoint_prior_scale": 0.001, "seasonality_prior_scale": 0.1},
            {"changepoint_prior_scale": 0.002, "seasonality_prior_scale": 0.15},
            {"changepoint_prior_scale": 0.0005, "seasonality_prior_scale": 0.1},
        ]

        for i, params in enumerate(prophet_params):
            try:
                model = Prophet(seasonality_mode='additive', **params)
                model.fit(prophet_df)
                future = model.make_future_dataframe(periods=forecast_horizon, freq='MS')
                forecast = model.predict(future)
                forecast['yhat'] = forecast['yhat'].clip(lower=0)

                temp = forecast[['ds', 'yhat']].rename(columns={'ds': 'DATE', 'yhat': 'Forecast'})
                temp['Model'] = f'Prophet_V{i+1}'

                for col in group_key._fields:
                    temp[col] = getattr(group_key, col)

                result_list.append(temp)
            except Exception as e:
                print(f"‚ö†Ô∏è Prophet_V{i+1} failed for {group_key}: {e}")

        return pd.concat(result_list, ignore_index=True) if result_list else None

    except Exception as e:
        print(f"‚ùå Error processing {group_key}: {e}")
        return None

# ========= Main Forecast Execution ========= #

from collections import namedtuple

GroupKey = namedtuple("GroupKey", ["COUNTRY", "CHANNEL", "PRODUCT_ID"])

# Reuse groups from SECTION 4
groups = [(GroupKey(*key), group.reset_index(drop=True)) for key, group in grouped_data]

print(f"üîÅ Generating forecasts for {len(groups)} product-country-channel combinations (Monthly Prophet)...")

with multiprocessing.Pool(processes=4) as pool:
    result_chunks = pool.starmap(forecast_single_group, groups)

forecast_results = [res for res in result_chunks if res is not None]

if forecast_results:
    all_model_forecasts = pd.concat(forecast_results, ignore_index=True)
    print("‚úÖ Forecasts generated for Prophet models.")
else:
    all_model_forecasts = pd.DataFrame()
    print("‚ö†Ô∏è No forecasts generated.")

"""# Section 6: Training Classifier

"""

# === SECTION 6: Training Classifier ===

# === Step 1: Generate all_model_metrics ===

# Ensure these are already loaded in your environment:
# - df: original actuals data
# - all_model_forecasts: all model outputs
# - groups: list of (GroupKey, group_df) from earlier sections

df['DATE'] = pd.to_datetime(df['DATE'])
all_model_forecasts['DATE'] = pd.to_datetime(all_model_forecasts['DATE'])

merged = pd.merge(
    all_model_forecasts,
    df[['DATE', 'COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'CY_QTY']],
    on=['DATE', 'COUNTRY', 'CHANNEL', 'PRODUCT_ID'],
    how='left'
)
merged = merged.dropna(subset=['CY_QTY'])

def compute_metrics(df):
    mape = np.mean(np.abs((df['CY_QTY'] - df['Forecast']) / (df['CY_QTY'] + 1e-5)))
    bias = (df['Forecast'].sum() - df['CY_QTY'].sum()) / (df['CY_QTY'].sum() + 1e-5)
    combined = mape + abs(bias)
    return pd.Series({"MAPE": mape, "Bias": bias, "Combined_Score": combined})

all_model_metrics = (
    merged.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'Model'])
    .apply(compute_metrics)
    .reset_index()
)

# === Step 2: Encode Labels ===

le_country = LabelEncoder()
le_channel = LabelEncoder()
le_product = LabelEncoder()
le_model = LabelEncoder()
le_best_model = LabelEncoder()

le_country.fit(all_model_metrics['COUNTRY'].unique())
le_channel.fit(all_model_metrics['CHANNEL'].unique())
le_product.fit(all_model_metrics['PRODUCT_ID'].unique())
le_model.fit(all_model_metrics['Model'].unique())
le_best_model.fit(all_model_metrics['Model'].unique())

# === Step 3: Extract Training Data ===

training_rows = []
unique_groups = all_model_metrics[['COUNTRY', 'CHANNEL', 'PRODUCT_ID']].drop_duplicates()

def extract_group_features(group_df):
    group_df = group_df.sort_values("DATE")
    y = group_df['CY_QTY'].values
    if len(y) < 2:
        return 0, 0, 0
    x = np.arange(len(y))
    slope = linregress(x, y).slope
    volatility = np.std(y) / (np.mean(y) + 1e-5)
    group_df['Month'] = group_df['DATE'].dt.month
    seasonality = group_df.groupby('Month')['CY_QTY'].std().mean() if group_df['Month'].nunique() > 1 else 0
    return slope, volatility, seasonality

for _, row in unique_groups.iterrows():
    group_key = (row['COUNTRY'], row['CHANNEL'], row['PRODUCT_ID'])
    key_metrics = all_model_metrics[
        (all_model_metrics['COUNTRY'] == group_key[0]) &
        (all_model_metrics['CHANNEL'] == group_key[1]) &
        (all_model_metrics['PRODUCT_ID'] == group_key[2])
    ]

    if key_metrics.empty:
        continue

    best_model = key_metrics.loc[key_metrics['Combined_Score'].idxmin(), 'Model']

    matching_group = [g for g in groups if (g[0].COUNTRY, g[0].CHANNEL, g[0].PRODUCT_ID) == group_key]
    if not matching_group:
        continue

    group_df = matching_group[0][1]
    slope, volatility, seasonality = extract_group_features(group_df)

    for _, mrow in key_metrics.iterrows():
        features = [
            le_country.transform([group_key[0]])[0],
            le_channel.transform([group_key[1]])[0],
            le_product.transform([group_key[2]])[0],
            le_model.transform([mrow['Model']])[0],
            mrow['MAPE'],
            mrow['Bias'],
            mrow['Combined_Score'],
            slope,
            volatility,
            seasonality
        ]
        label = le_best_model.transform([best_model])[0]
        training_rows.append((features, label))

# === Step 4: Train Classifier ===

X = [row[0] for row in training_rows]
y = [row[1] for row in training_rows]

if len(X) == 0:
    print("‚ö†Ô∏è No training data available.")
else:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"‚úÖ Meta Learner Trained. Validation Accuracy: {acc:.2%}")

    # Save model
    joblib.dump(clf, "ai_meta_model.pkl")
    print("üíæ Model saved as ai_meta_model.pkl")

"""# Section 7: Model Selection with AI Meta Learner"""

# === SECTION 7: Model Selection with AI Meta Learner ===


# Group Key definition (if not already)
GroupKey = namedtuple("GroupKey", ["COUNTRY", "CHANNEL", "PRODUCT_ID"])

# === Step 1: Generate MAPE, Bias, Combined Score ===

# Merge actuals with forecasts for metric evaluation
actuals_df = df.copy()  # original demand data
actuals_df['DATE'] = pd.to_datetime(actuals_df['DATE'])
forecast_df = all_model_forecasts.copy()
forecast_df['DATE'] = pd.to_datetime(forecast_df['DATE'])

# Merge on all keys + date
merged = pd.merge(
    forecast_df,
    actuals_df[['DATE', 'COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'CY_QTY']],
    on=['DATE', 'COUNTRY', 'CHANNEL', 'PRODUCT_ID'],
    how='left'
)

# Drop any rows where actuals are missing
merged = merged.dropna(subset=['CY_QTY'])

# Metric calculations
def compute_metrics(df):
    mape = np.mean(np.abs((df['CY_QTY'] - df['Forecast']) / (df['CY_QTY'] + 1e-5)))
    bias = (df['Forecast'].sum() - df['CY_QTY'].sum()) / (df['CY_QTY'].sum() + 1e-5)
    combined = mape + abs(bias)
    return pd.Series({"MAPE": mape, "Bias": bias, "Combined_Score": combined})

all_model_metrics = (
    merged.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'Model'])
    .apply(compute_metrics)
    .reset_index()
)

# === Step 2: Prepare Label Encoders ===

le_country = LabelEncoder()
le_channel = LabelEncoder()
le_product = LabelEncoder()
le_model = LabelEncoder()
le_best_model = LabelEncoder()

le_country.fit(all_model_metrics['COUNTRY'].unique())
le_channel.fit(all_model_metrics['CHANNEL'].unique())
le_product.fit(all_model_metrics['PRODUCT_ID'].unique())
le_model.fit(all_model_metrics['Model'].unique())
le_best_model.fit(all_model_metrics['Model'].unique())

# === Step 3: Load Classifier (or Dummy if not trained) ===

clf = joblib.load('ai_meta_model.pkl')  # Placeholder; update to load .pkl if available
# clf = joblib.load('ai_meta_model.pkl')

# === Step 4: Define AI Model Selector ===

def extract_group_features(group_df):
    group_df = group_df.sort_values("DATE")
    y = group_df['CY_QTY'].values
    if len(y) < 2:
        return 0, 0, 0
    x = np.arange(len(y))
    slope = linregress(x, y).slope
    volatility = np.std(y) / (np.mean(y) + 1e-5)
    group_df['Month'] = group_df['DATE'].dt.month
    seasonality = group_df.groupby('Month')['CY_QTY'].std().mean() if group_df['Month'].nunique() > 1 else 0
    return slope, volatility, seasonality

def select_best_model_ai(group_key, group_df, group_metrics):
    slope, volatility, seasonality = extract_group_features(group_df)

    feature_rows = []
    for _, row in group_metrics.iterrows():
        features = [
            le_country.transform([group_key.COUNTRY])[0],
            le_channel.transform([group_key.CHANNEL])[0],
            le_product.transform([group_key.PRODUCT_ID])[0],
            le_model.transform([row['Model']])[0],
            row['MAPE'],
            row['Bias'],
            row['Combined_Score'],
            slope,
            volatility,
            seasonality
        ]
        feature_rows.append((features, row['Model']))

    X = [f[0] for f in feature_rows]

    if hasattr(clf, "predict_proba"):
        probs = clf.predict_proba(X)  # shape: (n_samples, n_classes)

        if probs.shape[1] == 1:
            # Model only returned one class probability
            best_idx = np.argmax(probs[:, 0])
            confidence = probs[best_idx][0]
        else:
            # Multi-class case: take highest probability across all classes
            best_idx, best_class = np.unravel_index(np.argmax(probs), probs.shape)
            confidence = probs[best_idx, best_class]

        predicted_model = feature_rows[best_idx][1]
    else:
        # No probability method ‚Äî fallback to lowest Combined_Score
        predicted_model = group_metrics.loc[group_metrics['Combined_Score'].idxmin(), 'Model']
        confidence = 0

    if confidence > 0.80:
        return predicted_model, confidence
    else:
        best_row = group_metrics.loc[group_metrics['Combined_Score'].idxmin()]
        return best_row['Model'], confidence

# === Step 5: Save Selection to History for Online Learning ===

def save_to_history(group_key, row, slope, volatility, seasonality, file_path='model_performance_history.xlsx'):
    record = {
        'Date': pd.Timestamp.today().date(),
        'COUNTRY': group_key.COUNTRY,
        'CHANNEL': group_key.CHANNEL,
        'PRODUCT_ID': group_key.PRODUCT_ID,
        'Model': row['Model'],
        'MAPE': row['MAPE'],
        'Bias': row['Bias'],
        'Combined_Score': row['Combined_Score'],
        'Trend': slope,
        'Volatility': volatility,
        'Seasonality': seasonality
    }

    history_df = pd.DataFrame([record])
    try:
        if os.path.exists(file_path):
            existing = pd.read_excel(file_path)
            updated = pd.concat([existing, history_df], ignore_index=True)
        else:
            updated = history_df
        updated.to_excel(file_path, index=False)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not save to history: {e}")

# === Step 6: Run Model Selection for Each Group ===

final_best_models = []

for group_key, group_df in groups:
    group_metrics = all_model_metrics[
        (all_model_metrics['COUNTRY'] == group_key.COUNTRY) &
        (all_model_metrics['CHANNEL'] == group_key.CHANNEL) &
        (all_model_metrics['PRODUCT_ID'] == group_key.PRODUCT_ID)
    ]
    if group_metrics.empty:
        continue

    selected_model, confidence = select_best_model_ai(group_key, group_df, group_metrics)
    slope, volatility, seasonality = extract_group_features(group_df)
    best_row = group_metrics[group_metrics['Model'] == selected_model].iloc[0]

    save_to_history(group_key, best_row, slope, volatility, seasonality)
    best_row['Selected_Model'] = selected_model
    best_row['AI_Confidence'] = confidence
    final_best_models.append(best_row)

# === Step 7: Create Final Model Selection Output ===

best_model_df = pd.DataFrame(final_best_models)

print("‚úÖ Model selection completed for all groups.")

"""# Section 8: Final In-Market Forecast Output"""

from pandas.tseries.offsets import DateOffset

# Step 1: Define Forecast Range
last_actual_date = df['DATE'].max()
forecast_start_date = pd.to_datetime(f"{last_actual_date.year}-{last_actual_date.month}-01") + DateOffset(months=1)
forecast_end_date = (forecast_start_date + DateOffset(months=23)) + pd.offsets.MonthEnd(0)  # Include full last month

# Step 2: Merge selected models with forecasts
final_forecast_list = []

for idx, row in best_model_df.iterrows():
    key_filter = (
        (all_model_forecasts['COUNTRY'] == row['COUNTRY']) &
        (all_model_forecasts['CHANNEL'] == row['CHANNEL']) &
        (all_model_forecasts['PRODUCT_ID'] == row['PRODUCT_ID']) &
        (all_model_forecasts['Model'] == row['Selected_Model'])
    )
    model_forecast = all_model_forecasts[key_filter].copy()

    # Normalize DATE to first of month
    model_forecast['DATE'] = pd.to_datetime(model_forecast['DATE']).dt.to_period('M').dt.to_timestamp()

    # Filter to fixed 24-month window
    model_forecast = model_forecast[
        (model_forecast['DATE'] >= forecast_start_date) &
        (model_forecast['DATE'] <= forecast_end_date)
    ]

    final_forecast_list.append(model_forecast)

# Step 3: Combine forecasts
if final_forecast_list:
    final_inmarket_forecast = pd.concat(final_forecast_list, ignore_index=True)
    final_inmarket_forecast = final_inmarket_forecast.sort_values(by=['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE'])
    print("‚úÖ Final in-market forecast generated.")
else:
    final_inmarket_forecast = pd.DataFrame()
    print("‚ö†Ô∏è No forecast found for selected models.")

# Step 4: Format and merge with historicals
historical_data = df[['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE', 'CY_QTY']].copy()
historical_data.rename(columns={'CY_QTY': 'VALUE'}, inplace=True)
historical_data['TYPE'] = 'In Market History'

# Normalize forecast column
final_inmarket_forecast.rename(columns={'Forecast': 'VALUE'}, inplace=True)

forecast_only = final_inmarket_forecast[['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE', 'VALUE']].copy()
forecast_only['TYPE'] = 'In Market Forecast'

# Step 5: Combine and save to multi-sheet Excel file
final_combined = pd.concat([historical_data, forecast_only], ignore_index=True)
final_combined = final_combined.sort_values(by=['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE'])

"""# Section 9: To-Market Forecast Conversion"""

# === SECTION 9: Convert In-Market to To-Market Forecast (Dynamic Start with SOURCE_TYPE) ===

from dateutil.relativedelta import relativedelta

# --- Step 1: Load and Clean SOH Data (Assume July Closing) ---
soh_df = pd.read_excel("/content/drive/My Drive/Sample Files/In Market Stock Data/Distributor Stock.XLSX")
soh_df.columns = soh_df.columns.str.strip()

soh_df = soh_df[['ALT COUNTRY KEY', 'JP CUST TYPE', 'Product ID', 'Stock Quantity']].copy()
soh_df.rename(columns={
    'ALT COUNTRY KEY': 'COUNTRY',
    'JP CUST TYPE': 'CHANNEL',
    'Product ID': 'PRODUCT_ID',
    'Stock Quantity': 'SOH'
}, inplace=True)
soh_df = soh_df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID'], as_index=False).agg({'SOH': 'sum'})

# --- Step 2: Prepare IMS (History + Forecast) ---
ims_df = final_combined.copy()  # contains both history + forecast + TYPE column
ims_df = ims_df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'TYPE', 'DATE'], as_index=False).agg({'VALUE': 'sum'})
ims_df['DATE'] = pd.to_datetime(ims_df['DATE'])
ims_df['DATE'] = ims_df['DATE'].dt.to_period('M').dt.to_timestamp()
ims_df = ims_df.sort_values(['COUNTRY', 'CHANNEL', 'PRODUCT_ID', 'DATE'])

# --- Step 3: Parameters ---
default_target = 3
detailed_records = []

# --- Step 4: Loop Through Each Combination ---
for (country, channel, product), group in ims_df.groupby(['COUNTRY', 'CHANNEL', 'PRODUCT_ID']):
    group = group.sort_values('DATE').reset_index(drop=True)

    # Determine forecast start month for this product-country-channel
    forecast_rows = group.loc[group['TYPE'] == 'In Market Forecast', 'DATE']

    if forecast_rows.empty or pd.isna(forecast_rows.min()):
        continue  # No forecast data for this combination

    forecast_start_month = forecast_rows.min()
    soh_month = forecast_start_month - relativedelta(months=1)
    # Fetch SOH for the month before forecast start
    stock_row = soh_df[
        (soh_df['COUNTRY'] == country) &
        (soh_df['CHANNEL'] == channel) &
        (soh_df['PRODUCT_ID'] == product)
    ]
    starting_soh = stock_row['SOH'].values[0] if not stock_row.empty else 0

    # Prepend virtual row for month before first date in group
    first_month_in_data = group['DATE'].min()
    virtual_soh_row = pd.DataFrame([{
        'COUNTRY': country,
        'CHANNEL': channel,
        'PRODUCT_ID': product,
        'DATE': first_month_in_data - relativedelta(months=1),
        'VALUE': 0,
        'TYPE': 'Virtual SOH Row'
    }])
    group = pd.concat([virtual_soh_row, group], ignore_index=True).reset_index(drop=True)

    # Initialize SOH from Month-1 closing
    projected_soh = starting_soh

    for idx in range(1, len(group)):
        current_date = group.loc[idx, 'DATE']
        ims_value = group.loc[idx, 'VALUE']

        # Rolling average: 3 months before + 3 months after (excluding current month)
        window_months = [current_date - relativedelta(months=i) for i in [3, 2, 1]] + \
                        [current_date + relativedelta(months=i) for i in [0, 1, 2]]

        rolling_values = group[group['DATE'].isin(window_months)]['VALUE']
        rolling_avg = rolling_values.mean()
        target_stock = rolling_avg * default_target
        total_required = ims_value + target_stock
        replenish = max(total_required - projected_soh, 0)


        # Record KPIs only from forecast start month onwards
        if current_date >= forecast_start_month:

            detailed_records.extend([
                {
                    'COUNTRY': country,
                    'CHANNEL': channel,
                    'PRODUCT_ID': product,
                    'DATE': current_date,
                    'TYPE': 'Rolling Average',
                    'VALUE': round(rolling_avg, 0),
                },
                {
                    'COUNTRY': country,
                    'CHANNEL': channel,
                    'PRODUCT_ID': product,
                    'DATE': current_date,
                    'TYPE': 'Target Stock',
                    'VALUE': round(target_stock, 0)
                },
                {
                    'COUNTRY': country,
                    'CHANNEL': channel,
                    'PRODUCT_ID': product,
                    'DATE': current_date - relativedelta(months=1) ,
                    'TYPE': 'Projected SOH',
                    'VALUE': round(projected_soh, 0)
                },
                {
                    'COUNTRY': country,
                    'CHANNEL': channel,
                    'PRODUCT_ID': product,
                    'DATE': current_date,
                    'TYPE': 'To-Market Forecast',
                    'VALUE': round(replenish, 0)
                }
            ])
            projected_soh = projected_soh - ims_value + replenish

# --- Final Output ---
to_market_df = pd.DataFrame(detailed_records)
print("‚úÖ Final To-Market forecast.")

"""# Section 10: Download Forecast & Master Data"""

# Save forecasts + master data into one Excel file
output_path = "Final_Forecast_With_Master.xlsx"
with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
    master_enriched.to_excel(writer, sheet_name='Master_Data', index=False)
    final_combined.to_excel(writer, sheet_name='In-Market Forecast_Data', index=False)
    to_market_df.to_excel(writer, sheet_name='To-Market Forecast_Data', index=False)

print(f"üíæ Final Forecast with master data saved to {output_path}")

to_market_df[to_market_df['TYPE']=="Target"]